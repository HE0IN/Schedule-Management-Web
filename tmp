def res_api(
    api_url: str,
    proxies: Dict[str, Optional[str]],
    api_key: str,
    messages: List[Dict[str, str]],
    generate_kwargs: Dict[str, Any],
    stream: bool = False,
):
    """
    SOLA 가이드에서 제공한 res_api 그대로 구현.
    """
    data = {
        "api_key": api_key,
        "messages": messages,
        "generate_kwargs": generate_kwargs,
    }
    return requests.post(
        api_url,
        data=json.dumps(data, ensure_ascii=False),
        proxies=proxies,
        stream=stream,
    )


def call_llm(
    messages: List[Dict[str, str]],
    temperature: float = 0.3,
    max_tokens: int = 4086,
    top_p: float = 0.95,
    stream: bool = False,
) -> str:
    """
    SOLA API 스펙에 맞춘 LLM 호출 함수.
    - messages: SOLA 가이드의 messages 형식 그대로 사용
    - generate_kwargs: SOLA 가이드 형식 그대로 구성
    """
    if not API_URL or not API_KEY:
        raise RuntimeError("API_URL 또는 API_KEY가 .env에 설정되지 않았습니다.")

    generate_kwargs = {
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": top_p,
    }

    try:
        response = res_api(
            API_URL,
            proxies,
            API_KEY,
            messages,
            generate_kwargs,
            stream=stream,
        )
        response.raise_for_status()
    except Exception as e:
        logging.exception(f"SOLA LLM API 호출 실패: {e}")
        raise

    # SOLA Non-Stream 예제 기준: response.text 에 결과가 그대로 들어있다고 가정
    return response.text.strip()
